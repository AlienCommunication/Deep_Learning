{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF_TensorRT model.ipynb",
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhMuwJ6jiT_M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Import the libraries\n",
        "\"\"\"\n",
        "import tensorflow as tf\n",
        "import tensorflow.contrib.tensorrt as trt\n",
        "from tensorflow.python.platform import gfile\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Setting the session for TensorRT Optimisation\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "with tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.50))) as sess:\n",
        "    \"\"\"\n",
        "    importing the .meta file of tensorflow model\n",
        "    \"\"\"\n",
        "    saver = tf.train.import_meta_graph(\"/content/drive/My Drive/SelfPractise/TensorRT_Optimisation/TensorRT_Optimisation.meta\")\n",
        "    \"\"\"\n",
        "    Restore the weights to the meta Graph\n",
        "    \"\"\"\n",
        "    saver.restore(sess, \"/content/drive/My Drive/SelfPractise/TensorRT_Optimisation\")\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    Now Here We would like to specify which tensor output we want to use, To get it you can see the model's graph\n",
        "\n",
        "    \"\"\"\n",
        "    your_outputs = [\"output_tensor/Softmax\"]\n",
        "    \n",
        "    \"\"\"\n",
        "    Converting it into a Frozen model .pb file\n",
        "\n",
        "    \"\"\"\n",
        "    frozen_graph = tf.graph_util.convert_variables_to_constants(\n",
        "        sess, \n",
        "        tf.get_default_graph().as_graph_def(),# graph+weight from the session\n",
        "        output_node_names=your_outputs)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    Let's write the TensorRT Model which can be used later\n",
        "    \"\"\"\n",
        "    with gfile.FastGFile(\"./model/frozen_model.pb\", 'wb') as f:\n",
        "        f.write(frozen_graph.SerializeToString())\n",
        "    print(\"Frozen model is successfully stored!\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}